# Meeting Intelligence Agent (MIA)

A sophisticated multi-agent system for extracting structured insights from meeting transcripts using LangChain, LangSmith, and configurable LLM providers (Ollama, HuggingFace).

## Features

### Core Extraction Capabilities
- **Multi-format Support**: Upload transcripts in TXT, JSON, or SRT format
- **Advanced Preprocessing**: Filler removal, speaker normalization, topic segmentation
- **Flexible Model Strategy**: Choose between Ollama (Llama3) or HuggingFace models
- **Structured Extraction**: Automatically extract decisions, action items, risks, and owners
- **Confidence Scoring**: Each extracted item includes a confidence score
- **Export Options**: Download results as JSON or Markdown

### Evaluation & Quality Assurance (LangChain/LangSmith Integration)
- **Automated LLM-as-a-Judge Evaluation**: Uses Llama3 (via Ollama) or HuggingFace models to evaluate extraction quality
- **Multi-dimensional Scoring**: Evaluates coverage, factuality, clarity, completeness, specificity
- **Human-in-the-Loop Review**: Interactive interface for human reviewers to score and provide feedback
- **Evaluation Aggregation**: Combines LLM, human, and metric-based evaluations with confidence scoring
- **Continuous Improvement**: Tracks low-scoring extractions for retraining
- **LangSmith Observability**: Full tracing and monitoring of evaluation runs
- **Cost Optimization**: Intelligent caching and model fallback strategies

## Tech Stack

### Backend
- **Framework**: Python 3.10+, FastAPI
- **LLM Integration**: LangChain, LangSmith for observability
- **Model Providers**: 
  - Ollama (Llama3.2/Llama3) - Primary evaluation and inference
  - HuggingFace Transformers - Local and remote models
  - Extensible architecture for future providers (OpenAI, Anthropic, etc.)
- **ML Libraries**: Sentence Transformers, scikit-learn, ROUGE, BERTScore
- **Data Processing**: Pandas, NumPy
- **Validation**: Pydantic

### Frontend
- React 18
- TypeScript
- Vite
- Tailwind CSS
- shadcn-ui

## Setup Instructions

### Prerequisites

- Python 3.10 or higher
- Node.js 18+ and npm
- **Ollama** (for Llama3 models) - Install from [ollama.ai](https://ollama.ai)
- Hugging Face account with API token (for remote/hybrid strategies)
- **Optional**: LangSmith API key (for evaluation observability)

### Backend Setup

1. **Clone and navigate to backend directory:**
   ```bash
   cd backend
   ```

2. **Create and activate virtual environment:**
   ```bash
   # Windows
   py -m venv venv
   venv\Scripts\activate

   # Unix/MacOS
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Create `.env` file in backend directory:**
   ```env
   # Core Models
   HUGGINGFACE_TOKEN=your_token_here
   MODEL_STRATEGY=hybrid
   UPLOAD_DIR=./uploads
   OUTPUT_DIR=./outputs
   MAX_FILE_SIZE_MB=50
   SUMMARIZATION_MODEL=philschmid/bart-large-cnn-samsum
   NER_MODEL=dslim/bert-base-NER
   EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
   
   # Evaluation Models (LangChain Integration)
   EVALUATION_MODEL_PROVIDER=ollama  # ollama or huggingface
   EVALUATION_MODEL_NAME=llama3.2
   EVALUATION_MODEL_FALLBACK=llama3
   EVALUATION_TEMPERATURE=0.1
   OLLAMA_BASE_URL=http://localhost:11434
   
   # LangSmith Observability (Optional)
   LANGSMITH_API_KEY=your_langsmith_api_key
   LANGSMITH_PROJECT_NAME=mia-evaluations
   ```

5. **Setup Ollama (for evaluation models):**
   ```bash
   # Install Ollama from https://ollama.ai
   
   # Pull the required models
   ollama pull llama3.2
   ollama pull llama3  # fallback model
   
   # Verify Ollama is running
   ollama list
   ```

6. **Start the backend server:**
   ```bash
   uvicorn app.main:app --reload
   ```

   The API will be available at `http://localhost:8000`
   API documentation: `http://localhost:8000/docs`

### Frontend Setup

1. **Navigate to project root (where package.json is located)**

2. **Install dependencies:**
   ```bash
   npm install
   ```

3. **Create `.env` file in project root:**
   ```env
   VITE_API_BASE_URL=http://localhost:8000
   ```

4. **Start the development server:**
   ```bash
   npm run dev
   ```

   The frontend will be available at `http://localhost:5173`

## Documentation

- `docs/guides/`: Hands-on runbooks for setup, testing, troubleshooting, and performance tuning.
- `docs/reference/`: Architecture deep-dives, model notes, prompt fixes, and optimization plans.
- `docs/reports/`: Decision logs, planning outputs, accelerator summaries, and related meeting artifacts.
- `meeting_transcripts/`: Raw transcript inputs retained separately from curated documentation.

## Architecture Overview

### MIA System Flow

The Meeting Intelligence Agent processes transcripts through a multi-stage pipeline with comprehensive evaluation and human feedback loops:

```mermaid
graph TB
    A[ğŸ“„ Transcript Upload] --> B[ğŸ”§ Preprocessing]
    B --> C[ğŸ§  MIA Extraction Pipeline]
    C --> D[ğŸ“Š Results Display]
    C --> E[ğŸ” Automated Evaluation]
    E --> F[ğŸ“ˆ LangSmith Logging]
    E --> G[ğŸ‘¤ Human Review]
    G --> H[ğŸ”„ Evaluation Aggregation]
    H --> F
    H --> I[ğŸ“‹ Improvement Reports]
    I --> J[ğŸ”„ Model Retraining]

    subgraph "Extraction Components"
        C1[ğŸ“ Summary Generation]
        C2[âš–ï¸ Decision Extraction]
        C3[âœ… Action Items]
        C4[âš ï¸ Risk Identification]
    end
    C --> C1
    C --> C2
    C --> C3
    C --> C4

    subgraph "Evaluation Framework"
        E1[ğŸ¤– LLM Judge<br/>Llama3.2]
        E2[ğŸ“Š Traditional Metrics<br/>ROUGE, BERTScore]
        E3[ğŸ‘¥ Human Reviewers]
    end
    E --> E1
    E --> E2
    G --> E3

    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style E fill:#fff3e0
    style F fill:#e8f5e8
```

## Usage

1. **Upload a transcript file** (TXT, JSON, or SRT format)
2. **Configure settings:**
   - Model Strategy: Choose local, remote, or hybrid
   - Preprocessing: Choose basic or advanced
3. **Click "Process"** to start analysis
4. **View results** in the output panel with evaluation tabs
5. **Review quality scores** in the evaluation dashboard
6. **Provide human feedback** through the review interface
7. **Export results** as JSON or Markdown

## Extraction and Evaluation Architecture

### Core Extraction Pipeline

The meeting extraction that appears in the UI is produced through the `MeetingExtractor` pipeline defined in `backend/app/extraction/extractor.py`. The system extracts four main components: **Summary**, **Decisions**, **Action Items**, and **Risks**.

### Meeting Summary Pipeline

1. **Transcript preprocessing** (`backend/app/preprocessing/cleaner.py`):
   - `TranscriptCleaner.process` removes greetings, small talk, and filler speech.
   - Consecutive short turns from the same speaker are merged for better context.
   - Speaker names are normalized and (optionally) segments are grouped by topic using embeddings.
   ```text
   Before: "Hey everyone, uh yeah just wanted to kick things off... okay so I think launch is October 15."
   After:  "Just wanted to kick things off. I think launch is October 15."
   ```

2. **Hierarchical summarization** (`MeetingExtractor.extract_summary`):
   - Cleansed transcript is chunked into ~500â€“700 token windows using semantic chunking when embeddings are available, otherwise a word-based fallback.
   - Each chunk is summarized through `ModelAdapter.summarize`.
   - `_hierarchical_summarize` recursively combines partial summaries into a single narrative.
   ```text
   Before: 1,200-word raw discussion with repeated status updates.
   After:  "The team reviewed launch blockers, confirmed feature readiness, and aligned on timeline adjustments."
   ```

3. **Structured reasoning support** (`MeetingExtractor.extract_structured_data`):
   - Specialized extractors (decisions, actions, risks) in `backend/app/extraction/specialized_extractors.py` analyze the same segments.
   - Provenance tracking anchors each item to supporting transcript spans.
   - Semantic confidence scoring and validation prune low-quality signals and push results to the frontend.
   ```text
   Before: "Weâ€™ll push beta two weeks and Alex owns the risk mitigation."
   After:  Decision â†’ "Push beta to Oct 29"; Action â†’ "Alex to drive mitigation plan"; Risk â†’ "Timeline slip if QA fails."
   ```

4. **Executive summary synthesis** (`MeetingExtractor._synthesize_executive_summary`):
   - The initial narrative summary, structured artifacts, and derived metadata (meeting type, duration estimate, main topic) are combined.
   - A templated prompt re-invokes `ModelAdapter.summarize` to ensure required factual elements (dates, metrics, before/after decisions) are present.
   - Result is a polished two to three paragraph executive summary ready for the UI.
   ```text
   Before: "Discussed launch plan and risk mitigation."
   After:  "The planning session was a 45-minute review of the launch program, confirming the move from Oct 15 to Oct 29, highlighting four critical blockers, and assigning three mitigation owners."
   ```

5. **Quality gating** (`MeetingExtractor.process`):
   - Redundancy, coverage, and confidence metrics are calculated for the synthesized summary.
   - If thresholds are not met, the system falls back to the hierarchical summary to avoid hallucinated structure.
   - The UI receives both the final summary and a quality warning flag when confidence is low.

   This pipeline ensures that the displayed meeting summary is grounded in the cleaned transcript, preserves quantitative details, and aligns with the structured outputs surfaced elsewhere in the app.

### Decisions Extraction Pipeline

The decisions extraction process (`backend/app/extraction/specialized_extractors.py`) identifies concrete decisions made during meetings:

1. **Enhanced Decision Detection** (`EnhancedDecisionExtractor.extract_decisions`):
   - Uses semantic analysis to identify decision-making language patterns
   - Employs confidence scoring based on contextual indicators
   ```text
   Before: "I think we should probably go with option A, what do you all think?"
   After:  Decision â†’ "Selected option A" (Confidence: 0.7) + Rationale + Timeline context
   ```

2. **Multi-stage Validation** (`DecisionValidator.validate_decision`):
   - Checks for specificity, actionability, and temporal grounding
   - Filters out discussions vs. actual decisions
   ```text
   Before: "We discussed the budget and timeline extensively"
   After:  [Filtered out - no concrete decision identified]
   ```

3. **Provenance Linking** (`ProvenanceTracker.link_to_segments`):
   - Maps each decision back to specific transcript segments
   - Provides explainability for extraction confidence
   ```text
   Decision: "Move launch to Q2 2024" â†’ Linked to segments [15:30-16:45] with speaker attribution
   ```

### Action Items Extraction Pipeline

Action items extraction (`ActionItemExtractor.extract_action_items`) identifies specific tasks and assignments:

1. **Task Identification**:
   - Detects action-oriented language: "will", "should", "needs to", "assigned to"
   - Separates commitments from general discussions
   ```text
   Before: "Sarah mentioned she might look into the API integration sometime next week"
   After:  Action â†’ "Sarah to investigate API integration" (Deadline: next week, Confidence: 0.6)
   ```

2. **Owner and Timeline Extraction**:
   - Uses NER and temporal extraction to identify assignees and deadlines
   - Handles implicit and explicit assignments
   ```text
   Before: "Let's have someone check the database performance by Friday"
   After:  Action â†’ "Check database performance" (Owner: [Unassigned], Deadline: Friday, Priority: Medium)
   ```

3. **Priority Classification**:
   - Infers urgency from context and language cues
   - Classifies as High, Medium, or Low priority
   ```text
   Before: "This is critical - we need to fix the security issue immediately"
   After:  Action â†’ "Fix security issue" (Priority: High, Urgency: Immediate)
   ```

### Risk Identification Pipeline

Risk extraction (`RiskExtractor.extract_risks`) identifies potential issues and concerns:

1. **Risk Pattern Recognition**:
   - Detects risk indicators: "concern", "problem", "issue", "blocker", "risk"
   - Distinguishes between current problems and potential risks
   ```text
   Before: "If the API changes, we might have compatibility issues with the mobile app"
   After:  Risk â†’ "API compatibility issues with mobile app" (Impact: High, Likelihood: Medium)
   ```

2. **Impact and Likelihood Assessment**:
   - Uses contextual analysis to estimate potential impact
   - Assesses probability based on discussion tone and context
   ```text
   Before: "The vendor might delay delivery, which would push our timeline back significantly"
   After:  Risk â†’ "Vendor delivery delay" (Impact: High, Likelihood: Medium, Mitigation: [Identified backup plans])
   ```

3. **Mitigation Tracking**:
   - Identifies proposed or discussed mitigation strategies
   - Links risks to related action items for resolution
   ```text
   Risk: "Database performance degradation" â†’ Linked to Action: "Database performance audit by John"
   ```

### LangChain/LangSmith Evaluation Framework

The evaluation system (`backend/app/evaluation/`) uses LangChain to implement a comprehensive quality assessment framework:

#### 1. Automated LLM-as-a-Judge Evaluation

**Implementation**: `evaluation_chain.py` - `EvaluationChain`
```python
# Example evaluation prompt for summary assessment
prompt = """
You are an expert evaluator for meeting summaries. Given the original transcript 
and a generated summary, evaluate it based on these criteria:

- Coverage: How comprehensively the summary covers key discussion points
- Factuality: How accurately the summary reflects the actual conversation  
- Clarity: How clear and understandable the summary is

Return scores (0-10) and explanations for each criterion.
"""
```

**Model Configuration**: Uses Ollama with Llama3.2 by default, with fallback to HuggingFace models
```text
Before: Raw extraction results without quality assessment
After:  Each component scored on multiple dimensions with explanations
        Summary: Coverage=8.5, Factuality=9.0, Clarity=7.5 + detailed rationale
```

#### 2. Multi-Dimensional Scoring System

**Implementation**: `evaluation_schema.json` defines evaluation criteria
```json
{
  "summary": ["coverage", "factuality", "clarity"],
  "decisions": ["specificity", "completeness", "clarity"],
  "action_items": ["owner", "timeline", "clarity", "priority"],
  "risks": ["impact", "likelihood", "specificity"]
}
```

**Before/After Example**:
```text
Before: Basic confidence scores (0.7, 0.8, 0.9)
After:  Structured evaluation:
        - Decisions: Specificity=8.2, Completeness=7.8, Clarity=9.1
        - Human-interpretable quality assessment with improvement recommendations
```

#### 3. Human-in-the-Loop Review System

**Implementation**: `HumanReviewForm.tsx` + `evaluation_routes.py`
- Interactive sliders for criterion-based scoring
- Side-by-side comparison of extracted content vs. ground truth
- Free-text explanations and feedback collection
- "Mark for retraining" functionality for continuous improvement

```text
Before: No human feedback mechanism
After:  Reviewers can score each extraction component, provide explanations,
        and flag poor examples for model improvement
```

#### 4. Evaluation Aggregation and Confidence Scoring

**Implementation**: `evaluation_aggregator.py` - `EvaluationAggregator`
```python
# Weighted combination of LLM, human, and metric-based scores
final_score = 0.4 * llm_score + 0.4 * human_score + 0.2 * metrics_score
confidence = "high" if llm_human_agreement > 0.8 else "medium" if > 0.6 else "low"
```

**Before/After Example**:
```text
Before: Single confidence score per extraction
After:  Comprehensive evaluation:
        - Aggregate Score: 8.3/10 (High Confidence)  
        - Agreement Level: High (LLM-Human delta < 1.0)
        - Sources: [LLM, Human, Metrics] with transparent weights
```

#### 5. LangSmith Observability and Monitoring

**Implementation**: `evaluation_runner.py` with LangChain callbacks
- Automatic logging of all evaluation runs to LangSmith
- Performance tracking across model versions
- Cost monitoring and optimization insights
- Evaluation trend analysis and regression detection

```text
Before: No visibility into evaluation process or performance trends
After:  Full observability:
        - Evaluation runs tracked in LangSmith dashboard
        - Model performance trends over time
        - Cost analysis per evaluation type
        - Automated regression detection
```

#### 6. Traditional Metrics Integration

**Implementation**: `metrics_evaluator.py` - `MetricsEvaluator`
- **ROUGE Scores**: For summary quality assessment
- **BERTScore**: For semantic similarity measurement  
- **Precision/Recall/F1**: For extraction completeness
- **Coverage Metrics**: For information retention analysis

```text
Before: Only LLM-based subjective evaluation
After:  Combined evaluation approach:
        - ROUGE-L F1: 0.67 (summary overlap with reference)
        - BERTScore: 0.82 (semantic similarity)  
        - Decision Recall: 0.89 (completeness)
        - Overall Quality: 8.4/10 (aggregated score)
```

### Configuration-Driven Model Selection

The system supports plug-and-play model configuration across all components:

#### Evaluation Model Configuration
```bash
# Use Ollama/Llama3 (recommended)
EVALUATION_MODEL_PROVIDER=ollama
EVALUATION_MODEL_NAME=llama3.2

# Use HuggingFace models
EVALUATION_MODEL_PROVIDER=huggingface  
EVALUATION_MODEL_NAME=microsoft/DialoGPT-large

# Future extensibility: OpenAI support available but not currently used
# EVALUATION_MODEL_PROVIDER=openai
# EVALUATION_MODEL_NAME=gpt-4o-mini
```

#### Extraction Model Configuration
```bash
# Core extraction models
MODEL_STRATEGY=hybrid  # local, remote, hybrid, ollama
SUMMARIZATION_MODEL=philschmid/bart-large-cnn-samsum
NER_MODEL=dslim/bert-base-NER
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

This architecture ensures that changing model providers requires only configuration updates, with automatic fallback handling and comprehensive error recovery.

## Detailed System Workflows

### Evaluation Pipeline Flow

The evaluation system provides multi-layered quality assessment with automated scoring, human review, and continuous improvement:

```mermaid
sequenceDiagram
    participant UI as Frontend UI
    participant API as FastAPI Backend
    participant LLM as LLM Evaluator<br/>(Llama3.2)
    participant DB as Results Storage
    participant LS as LangSmith
    participant HR as Human Reviewer

    Note over UI,HR: Evaluation Workflow

    UI->>API: 1. POST /api/evaluation/{job_id}/trigger
    API->>DB: 2. Load extraction results
    
    par Automated Evaluation
        API->>LLM: 3a. Evaluate Summary (coverage, factuality, clarity)
        LLM-->>API: 3b. Return scores + explanations
        API->>LLM: 4a. Evaluate Decisions (specificity, completeness, clarity)
        LLM-->>API: 4b. Return scores + explanations
        API->>LLM: 5a. Evaluate Action Items (owner, timeline, clarity, priority)
        LLM-->>API: 5b. Return scores + explanations
        API->>LLM: 6a. Evaluate Risks (impact, likelihood, specificity)
        LLM-->>API: 6b. Return scores + explanations
    and Traditional Metrics
        API->>API: 7. Calculate ROUGE scores
        API->>API: 8. Calculate BERTScore
        API->>API: 9. Calculate precision/recall
    end

    API->>API: 10. Aggregate all evaluation scores
    API->>DB: 11. Save evaluation results
    API->>LS: 12. Log evaluation to LangSmith
    API-->>UI: 13. Return evaluation status

    Note over UI,HR: Human Review Process

    UI->>HR: 14. Display Human Review Form
    HR->>UI: 15. Submit scores + feedback
    UI->>API: 16. POST /api/evaluation/{job_id}/human-review
    API->>API: 17. Re-aggregate with human input
    API->>DB: 18. Update final scores
    API->>LS: 19. Log updated evaluation

    Note over API,LS: Continuous Improvement

    API->>API: 20. Generate improvement report
    API->>DB: 21. Flag low-scoring items for retraining
```

### Human-in-the-Loop Review Process

The human review system enables subject matter experts to provide feedback and improve model performance:

```mermaid
graph TB
    A[ğŸ“‹ Extraction Results] --> B[ğŸ¯ Automated Evaluation]
    B --> C{Quality Score<br/>< 7.0?}
    
    C -->|Yes| D[ğŸš¨ Flagged for<br/>Human Review]
    C -->|No| E[âœ… High Quality<br/>Auto-Approve]
    
    D --> F[ğŸ‘¤ Human Reviewer<br/>Interface]
    F --> G[ğŸ“Š Component Scoring]
    
    subgraph "Review Criteria"
        G1[ğŸ“ Summary<br/>â€¢ Coverage<br/>â€¢ Factuality<br/>â€¢ Clarity]
        G2[âš–ï¸ Decisions<br/>â€¢ Specificity<br/>â€¢ Completeness<br/>â€¢ Clarity]
        G3[âœ… Action Items<br/>â€¢ Owner Assignment<br/>â€¢ Timeline Clarity<br/>â€¢ Priority Accuracy]
        G4[âš ï¸ Risks<br/>â€¢ Impact Assessment<br/>â€¢ Likelihood<br/>â€¢ Specificity]
    end
    
    G --> G1
    G --> G2
    G --> G3
    G --> G4
    
    G1 --> H[ğŸ’¬ Reviewer Feedback]
    G2 --> H
    G3 --> H
    G4 --> H
    
    H --> I{Mark for<br/>Retraining?}
    I -->|Yes| J[ğŸ”„ Retraining Queue]
    I -->|No| K[ğŸ“ˆ Update Scores]
    
    K --> L[ğŸ”€ Score Aggregation]
    E --> L
    
    L --> M[ğŸ“Š Final Quality Score]
    M --> N[ğŸ“‹ Improvement Report]
    N --> O[ğŸ“ˆ LangSmith Dashboard]
    
    J --> P[ğŸ§  Model Improvement<br/>Process]
    P --> Q[ğŸ”„ Next Iteration]

    style D fill:#fff3cd
    style F fill:#d4edda
    style J fill:#f8d7da
    style O fill:#e2e3e5
```

### API Endpoints for Evaluation

The system provides RESTful API endpoints for evaluation management:

#### Core Evaluation Endpoints
```bash
# Trigger evaluation for a processed job
POST /api/evaluation/{job_id}/trigger
{
  "include_llm": true,
  "include_metrics": true,
  "reference_data": {...}  # optional ground truth
}

# Get evaluation status and results
GET /api/evaluation/{job_id}/status

# Submit human review
POST /api/evaluation/{job_id}/human-review  
{
  "component": "summary",
  "scores": {"coverage": 8, "factuality": 9, "clarity": 7},
  "explanations": {"coverage": "Missing key decision details"},
  "mark_for_retraining": false
}

# Get evaluation schema (for frontend)
GET /api/evaluation/schema

# Get model configuration info
GET /api/evaluation/model-info
```

#### Evaluation Dashboard Integration
```bash
# List evaluation jobs by status
GET /api/evaluation/jobs/{status}?limit=20&offset=0

# Batch evaluation for multiple jobs  
POST /api/evaluation/batch
{
  "job_ids": ["job1", "job2", "job3"],
  "parallel": true
}

# Get retraining candidates (flagged by reviewers)
GET /api/evaluation/retraining-candidates
```

### Usage Example: End-to-End Evaluation Flow

1. **Process a transcript** through the main MIA pipeline
2. **Trigger evaluation** automatically or manually
3. **Review results** in the evaluation dashboard
4. **Submit human feedback** through the review interface  
5. **Monitor trends** in LangSmith for continuous improvement

```python
# Example programmatic usage
import requests

# Step 1: Process transcript
response = requests.post("/api/process/{upload_id}")
job_id = response.json()["job_id"]

# Step 2: Trigger evaluation  
requests.post(f"/api/evaluation/{job_id}/trigger")

# Step 3: Check evaluation status
eval_status = requests.get(f"/api/evaluation/{job_id}/status").json()
print(f"Aggregate Score: {eval_status['aggregated_evaluation']['aggregate_score']}")

# Step 4: Submit human review
review_data = {
    "component": "summary", 
    "scores": {"coverage": 8, "factuality": 9, "clarity": 7},
    "explanations": {"coverage": "Good coverage of main points"}
}
requests.post(f"/api/evaluation/{job_id}/human-review", json=review_data)
```

## Model Strategy Options

- **Hybrid** (Recommended): Uses local models for extraction, HF API for summarization
- **Local**: Runs all models locally (requires GPU for best performance)
- **Remote**: Uses Hugging Face Inference API for all tasks

## Project Structure

```
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/               # FastAPI routes
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py      # Core MIA processing endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ model_management.py  # Model management endpoints  
â”‚   â”‚   â”‚   â””â”€â”€ evaluation_routes.py # Evaluation/review endpoints
â”‚   â”‚   â”œâ”€â”€ config/            # Configuration settings
â”‚   â”‚   â”‚   â”œâ”€â”€ settings.py    # Application settings
â”‚   â”‚   â”‚   â””â”€â”€ evaluation_schema.json  # Evaluation criteria schema
â”‚   â”‚   â”œâ”€â”€ extraction/        # Information extraction pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ extractor.py   # Main extraction orchestrator
â”‚   â”‚   â”‚   â”œâ”€â”€ specialized_extractors.py  # Component-specific extractors
â”‚   â”‚   â”‚   â”œâ”€â”€ enhanced_decision_extractor.py  # Advanced decision logic
â”‚   â”‚   â”‚   â”œâ”€â”€ provenance.py  # Transcript linking and attribution
â”‚   â”‚   â”‚   â””â”€â”€ validator.py   # Extraction quality validation
â”‚   â”‚   â”œâ”€â”€ evaluation/        # LangChain/LangSmith evaluation framework
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluation_chain.py      # LLM-as-judge evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluation_runner.py     # Evaluation orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluation_aggregator.py # Multi-source score aggregation
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics_evaluator.py     # Traditional metrics (ROUGE, BERTScore)
â”‚   â”‚   â”‚   â””â”€â”€ model_adapter.py         # Pluggable LLM providers
â”‚   â”‚   â”œâ”€â”€ models/            # Model adapters
â”‚   â”‚   â”‚   â”œâ”€â”€ adapter.py     # Core model abstraction
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_adapter.py  # Ollama/Llama integration
â”‚   â”‚   â”‚   â””â”€â”€ model_manager.py   # Model lifecycle management
â”‚   â”‚   â”œâ”€â”€ preprocessing/     # Transcript preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaner.py     # Text cleaning and normalization
â”‚   â”‚   â”‚   â”œâ”€â”€ enhanced_pipeline.py  # Advanced preprocessing
â”‚   â”‚   â”‚   â””â”€â”€ parser.py      # Multi-format transcript parsing
â”‚   â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”‚   â”‚   â””â”€â”€ storage.py     # File and result management
â”‚   â”‚   â””â”€â”€ main.py            # FastAPI app entry point
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ uploads/               # Uploaded transcript files
â”‚   â””â”€â”€ outputs/               # Processing results + evaluation reports
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/            # React components
â”‚   â”‚   â”œâ”€â”€ MIAOutput.tsx      # Main results display (with evaluation tabs)
â”‚   â”‚   â”œâ”€â”€ EvaluationDashboard.tsx    # Evaluation metrics visualization
â”‚   â”‚   â”œâ”€â”€ HumanReviewForm.tsx        # Human-in-the-loop review interface
â”‚   â”‚   â”œâ”€â”€ EnhancedDecisionCard.tsx   # Decision display with provenance
â”‚   â”‚   â””â”€â”€ [other components]
â”‚   â”œâ”€â”€ services/              # API service client
â”‚   â”‚   â””â”€â”€ miaService.ts      # Backend API integration
â”‚   â”œâ”€â”€ pages/                 # Page components
â”‚   â””â”€â”€ types/                 # TypeScript types
â”œâ”€â”€ meeting_transcripts/       # Sample transcripts for testing
â””â”€â”€ venv/                      # Python virtual environment (git-ignored)
```

## Development

### Running Tests

```bash
# Backend tests
cd backend
pytest

# Frontend tests (if configured)
npm test
```

### Building for Production

```bash
# Frontend
npm run build

# Backend (using Docker)
docker build -t mia-backend ./backend
```

## Notes

- Models are cached locally in `backend/models_cache/` to avoid repeated downloads
- First run may take longer as models are downloaded
- For best performance with local models, a GPU is recommended
- Free Hugging Face API tier has rate limits

## LangSmith Integration and Monitoring

LangSmith provides comprehensive observability for the MIA evaluation system, enabling continuous improvement and performance monitoring.

### LangSmith Dashboard Overview

```mermaid
graph TB
    subgraph "LangSmith Platform"
        A[ğŸ“ˆ Project Dashboard<br/>mia-evaluations]
        B[ğŸ” Run Tracking]
        C[ğŸ“Š Performance Analytics]
        D[ğŸ¯ Evaluation Trends]
        E[ğŸ’° Cost Monitoring]
        F[ğŸš¨ Error Detection]
    end

    subgraph "MIA System Integration"
        G[ğŸ§  Evaluation Runs]
        H[ğŸ“‹ Metadata Logging]
        I[â±ï¸ Performance Metrics]
        J[ğŸ”„ Feedback Loops]
    end

    G --> A
    G --> B
    H --> C
    I --> D
    J --> E
    
    A --> K[ğŸ‘ï¸ Real-time Monitoring]
    B --> L[ğŸ”— Trace Inspection]
    C --> M[ğŸ“ˆ Trend Analysis]
    D --> N[ğŸ¯ Quality Insights]
    
    style A fill:#e3f2fd
    style K fill:#e8f5e8
    style M fill:#fff3e0
    style N fill:#f3e5f5
```

### Setting Up LangSmith Monitoring

#### 1. **Initial Configuration**

```bash
# Add to your .env file
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_PROJECT_NAME=mia-evaluations

# Verify connection
curl -X GET "http://localhost:8000/api/evaluation/model-info"
```

#### 2. **What Gets Logged to LangSmith**

```mermaid
graph LR
    subgraph "Evaluation Data Flow"
        A[ğŸ§  LLM Evaluations] --> E[ğŸ“Š LangSmith Run]
        B[ğŸ“Š Traditional Metrics] --> E
        C[ğŸ‘¤ Human Reviews] --> E
        D[ğŸ”€ Aggregated Scores] --> E
    end

    subgraph "LangSmith Run Contains"
        E --> F[ğŸ“‹ Run Metadata<br/>â€¢ Job ID<br/>â€¢ Timestamp<br/>â€¢ Model Info]
        E --> G[ğŸ“Š Input Data<br/>â€¢ Original Transcript<br/>â€¢ Extraction Results<br/>â€¢ Reference Data]
        E --> H[ğŸ“ˆ Output Scores<br/>â€¢ Component Scores<br/>â€¢ Explanations<br/>â€¢ Confidence Levels]
        E --> I[ğŸ·ï¸ Tags<br/>â€¢ mia<br/>â€¢ evaluation<br/>â€¢ automated]
    end

    style E fill:#e1f5fe
    style F fill:#f3e5f5
    style G fill:#fff3e0
    style H fill:#e8f5e8
    style I fill:#fce4ec
```

### Using LangSmith for Continuous Improvement

#### Performance Monitoring Dashboard

1. **Navigate to LangSmith**: https://smith.langchain.com/
2. **Select Project**: `mia-evaluations`
3. **Monitor Key Metrics**:

```mermaid
graph TB
    A[ğŸ¯ Evaluation Quality Metrics] --> A1[ğŸ“Š Average Scores by Component]
    A --> A2[ğŸ“ˆ Score Trends Over Time]
    A --> A3[ğŸ¯ Pass/Fail Rate by Threshold]
    
    B[âš¡ Performance Metrics] --> B1[â±ï¸ Evaluation Latency]
    B --> B2[ğŸ”„ Throughput (Evaluations/hour)]
    B --> B3[ğŸ’° Cost per Evaluation]
    
    C[ğŸ” Quality Insights] --> C1[ğŸ“‹ Low-Scoring Patterns]
    C --> C2[ğŸš¨ Common Failure Points]
    C --> C3[ğŸ’¡ Improvement Opportunities]
    
    D[ğŸ¤– Model Performance] --> D1[ğŸ¯ LLM vs Human Agreement]
    D --> D2[ğŸ“Š Confidence Calibration]
    D --> D3[ğŸ”„ Model Drift Detection]

    style A fill:#e8f5e8
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e1f5fe
```

#### Identifying Improvement Opportunities

**Step-by-Step Analysis Process:**

1. **Filter by Low Scores**:
   ```
   LangSmith Filter: aggregate_score < 6.0
   ```

2. **Analyze Patterns**:
   - Which components consistently score low?
   - Are there specific meeting types with issues?
   - Do certain topics perform worse?

3. **Review LLM Explanations**:
   ```json
   {
     "explanations": {
       "coverage": "Summary misses key decisions discussed in minutes 15-20",
       "factuality": "Incorrect attribution of action item to wrong person"
     }
   }
   ```

4. **Cross-Reference with Human Reviews**:
   ```mermaid
   graph LR
       A[ğŸ¤– LLM Score: 5.2] --> C[â“ Disagreement Analysis]
       B[ğŸ‘¤ Human Score: 8.1] --> C
       C --> D[ğŸ” Root Cause Investigation]
       D --> E[ğŸ› ï¸ Prompt Refinement]
       D --> F[ğŸ“Š Model Retraining]
       D --> G[ğŸ“‹ Process Improvement]
   ```

### Advanced LangSmith Features for MIA

#### Custom Evaluations and A/B Testing

```mermaid
sequenceDiagram
    participant Dev as Developer
    participant LS as LangSmith
    participant MIA as MIA System
    participant Users as End Users

    Note over Dev,Users: A/B Testing Workflow

    Dev->>LS: 1. Create evaluation dataset
    Dev->>LS: 2. Define custom evaluators
    Dev->>MIA: 3. Deploy model variant A
    Users->>MIA: 4. Process transcripts (variant A)
    MIA->>LS: 5. Log results with tags=["variant_a"]
    
    Dev->>MIA: 6. Deploy model variant B
    Users->>MIA: 7. Process transcripts (variant B)
    MIA->>LS: 8. Log results with tags=["variant_b"]
    
    Dev->>LS: 9. Compare variant performance
    LS-->>Dev: 10. Statistical significance results
    Dev->>MIA: 11. Deploy winning variant
```

#### Setting Up Custom Evaluations

1. **Create Evaluation Dataset**:
   ```python
   # In LangSmith UI or via API
   evaluation_dataset = {
       "name": "mia-golden-set",
       "examples": [
           {
               "transcript": "Meeting transcript...",
               "expected_summary": "Gold standard summary...",
               "expected_decisions": [...]
           }
       ]
   }
   ```

2. **Monitor Model Performance**:
   ```bash
   # Trigger batch evaluation
   curl -X POST "http://localhost:8000/api/evaluation/batch" \
     -H "Content-Type: application/json" \
     -d '{"job_ids": ["job1", "job2", "job3"], "parallel": true}'
   ```

### Cost Optimization with LangSmith

```mermaid
graph TB
    A[ğŸ’° Cost Monitoring] --> B[ğŸ“Š Usage Analytics]
    B --> C[ğŸ¯ Optimization Strategies]
    
    subgraph "Cost Components"
        D[ğŸ¤– LLM Evaluation Calls]
        E[ğŸ“Š Traditional Metrics]
        F[ğŸ’¾ Storage Costs]
        G[ğŸ”„ API Requests]
    end
    
    B --> D
    B --> E
    B --> F
    B --> G
    
    C --> H[âš¡ Smart Caching<br/>Skip duplicate evaluations]
    C --> I[ğŸ¯ Selective Evaluation<br/>Focus on low-confidence items]
    C --> J[ğŸ“Š Batch Processing<br/>Reduce API overhead]
    C --> K[ğŸ”„ Model Fallbacks<br/>Use cheaper models when appropriate]
    
    style A fill:#fff3cd
    style H fill:#d4edda
    style I fill:#d4edda
    style J fill:#d4edda
    style K fill:#d4edda
```

### Troubleshooting LangSmith Integration

#### Common Issues and Solutions

| Issue | Symptoms | Solution |
|-------|----------|----------|
| ğŸš« No data in dashboard | Empty project view | Check API key configuration and network connectivity |
| âš ï¸ Incomplete runs | Missing evaluation data | Verify all evaluation components are completing successfully |
| ğŸ’° High costs | Unexpected billing | Enable caching and implement selective evaluation |
| ğŸŒ Slow performance | Long evaluation times | Use batch processing and parallel evaluation |

#### Monitoring Checklist

- [ ] âœ… Evaluation runs appearing in dashboard
- [ ] ğŸ“Š All metadata fields populated correctly  
- [ ] ğŸ¯ Quality scores within expected ranges
- [ ] ğŸ’° Cost tracking enabled and monitored
- [ ] ğŸš¨ Error alerts configured for failed evaluations
- [ ] ğŸ“ˆ Weekly performance review scheduled

## License

[Your License Here]
